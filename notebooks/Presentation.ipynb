{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Présentation des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir trouvé et implémenté les différentes méthodes de classification et d'analyse du sentiment autour d'une phrase, nous avons pu effectuer des tests de précision et de recall sur un ensemble de donnée pré labélisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultat de précision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.feature_extraction.text as skl_txt\n",
    "import sklearn.linear_model as skl_lm\n",
    "import sklearn.neighbors as skl_nei\n",
    "import sklearn.ensemble as skl_en\n",
    "import sklearn.naive_bayes as skl_nb\n",
    "from sklearn import metrics as skl_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_hutto = \"../data/hutto_ICWSM_2014/results/data_modified.txt\"\n",
    "\n",
    "data_files = [path_hutto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams = (1,1)\n",
    "binn = False\n",
    "idf = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    file_data = json.loads(f.readlines()[0])\n",
    "    f.close()\n",
    "    return np.array(file_data)\n",
    "\n",
    "def vectorize_data(text_data, ngrams, binn, idf):\n",
    "    if (binn == True):\n",
    "        idf = False\n",
    "    vectorizer = skl_txt.TfidfVectorizer(use_idf = idf, binary = binn, ngram_range = ngrams)\n",
    "    return vectorizer.fit_transform(text_data) \n",
    "\n",
    "def parse_data(file_data):\n",
    "    data, labels = file_data[:, 1], np.array(file_data[:, 0], dtype='float')\n",
    "    labels[labels > 0] = 1\n",
    "    labels[labels < 0] = 0\n",
    "    data = vectorize_data(data, ngrams, binn, idf)\n",
    "    return partition_data(data, labels)\n",
    "\n",
    "def partition_data(data, labels, ratio = 0.7):\n",
    "    N = int(ratio * data.shape[0])\n",
    "    idx = np.random.permutation(data.shape[0])\n",
    "    train_data = data[idx[:N]]\n",
    "    train_labels = labels[idx[:N]]\n",
    "    test_data = data[idx[N:]]\n",
    "    test_labels = labels[idx[N:]]\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def get_data(files):\n",
    "    data = []\n",
    "    for file_path in files:\n",
    "        file_data = parse_file(file_path)\n",
    "        tr_data, tr_labels, te_data, te_labels = parse_data(file_data)\n",
    "        partitioned_data = {\n",
    "            'train_data': tr_data,\n",
    "            'train_labels' : tr_labels,\n",
    "            'test_data': te_data,\n",
    "            'test_labels' : te_labels\n",
    "        }\n",
    "        data.append(partitioned_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(test_labels, predicted_labels):\n",
    "    acc = skl_metrics.accuracy_score(test_labels, predicted_labels)\n",
    "    recall = skl_metrics.recall_score(test_labels, predicted_labels)\n",
    "    pre = skl_metrics.precision_score(test_labels, predicted_labels)\n",
    "    return (pre, recall, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_performance(clf, data_train, train_labels, data_test, test_labels):\n",
    "    clf.fit(data_train, train_labels)\n",
    "    pr_lbls = clf.predict(data_test)\n",
    "    return get_metrics(test_labels, pr_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autolabels(rects, ax):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%d' % int(height * 100) + \"%\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "def plot_results(labels, results):\n",
    "    ind = np.arange(len(labels))  # the x locations for the groups\n",
    "    width = 0.22    # the width of the bars\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    rects1 = ax.bar(ind , results[:,0], width, color='g')\n",
    "    rects2 = ax.bar(ind - width, results[:,1], width, color='r')\n",
    "    rects3 = ax.bar(ind + width, results[:,2], width, color='b')\n",
    "    \n",
    "    # add some text for labels, title and axes ticks\n",
    "    ax.set_ylim(0,1.2)\n",
    "    ax.set_ylabel('Pourcentage %')\n",
    "    ax.set_title(\"Benchmark\")\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(labels, rotation = 'vertical')\n",
    "    ax.legend((rects1[0], rects2[0], rects3[0]), ('precision', 'recall', 'accuracy'))\n",
    "    \n",
    "    autolabels(rects1, ax)\n",
    "    autolabels(rects2, ax)\n",
    "    autolabels(rects3, ax)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(clfs, data):\n",
    "    results = []\n",
    "    train_data = data['train_data']\n",
    "    train_labels = data['train_labels']\n",
    "    test_data = data['test_data']\n",
    "    test_labels = data['test_labels']\n",
    "    for clf, name in clfs:\n",
    "        results.append(validate_performance(clf, train_data, train_labels, test_data, test_labels))\n",
    "    plot_results(clfs[:, 1], np.array(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = np.array([[skl_nb.MultinomialNB(alpha=.01), \"Multinomial NB\"],\n",
    "        [skl_nb.BernoulliNB(alpha=.01), \"BernoulliNB\"],\n",
    "        [skl_lm.RidgeClassifier(tol=1e-2, solver=\"sag\"), \"Ridge Classifier\"],\n",
    "        [skl_lm.Perceptron(max_iter=50), \"Perceptron\"], \n",
    "        [skl_lm.PassiveAggressiveClassifier(max_iter=50), \"Passive-Aggressive\"],\n",
    "        [skl_nei.KNeighborsClassifier(n_neighbors=10), \"kNN\"],\n",
    "        [skl_en.RandomForestClassifier(n_estimators=100), \"Random Forest\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = get_data(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-07a1b4561803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-c7bbb4baa30d>\u001b[0m in \u001b[0;36mbenchmark\u001b[0;34m(clfs, data)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "benchmark(clfs, data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
