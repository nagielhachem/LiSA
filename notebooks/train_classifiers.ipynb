{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import sklearn.feature_extraction.text as skl_txt\n",
    "import sklearn.linear_model as skl_lm\n",
    "import sklearn.neighbors as skl_nei\n",
    "import sklearn.ensemble as skl_en\n",
    "import sklearn.naive_bayes as skl_nb\n",
    "import sklearn.svm as skl_svm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Najj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocessor import Preprocessor as Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/data_cleaned/labelised/\"\n",
    "\n",
    "path_hutto = data_dir + \"hutto.txt\"\n",
    "path_airline = data_dir + \"airline.txt\"\n",
    "path_kaggle = data_dir + \"kaggle.txt\"\n",
    "path_michigan = data_dir + \"michigan.txt\"\n",
    "path_rtneg = data_dir + \"rt-polarity-neg.txt\"\n",
    "path_rtpos = data_dir + \"rt-polarity-pos.txt\"\n",
    "\n",
    "data_files = {\n",
    "    \"kaggle\" : path_kaggle, \n",
    "             }\n",
    "\n",
    "ngrams = (1,1)\n",
    "binn = False\n",
    "idf = True\n",
    "\n",
    "pre = Preprocessor()\n",
    "vectorizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    file_data = json.loads(f.readlines()[0])\n",
    "    f.close()\n",
    "    return np.array(file_data)\n",
    "\n",
    "def preprocess(text_data):\n",
    "    for i, tweet in enumerate(text_data):\n",
    "        t = pre.default_processing(tweet)\n",
    "        text_data[i] = t\n",
    "\n",
    "def vectorize_data(text_data, ngrams, binn, idf):\n",
    "    if (binn == True):\n",
    "        idf = False\n",
    "    vectorizer = skl_txt.TfidfVectorizer(use_idf = idf, binary = binn, ngram_range = ngrams)\n",
    "    print(vectorizer)\n",
    "    vectorizer.fit(text_data) \n",
    "    return vectorizer.transform(text_data)\n",
    "\n",
    "def parse_data(file_data):\n",
    "    data, labels = file_data[:, 1], np.array(file_data[:, 0], dtype='int')\n",
    "    preprocess(data)\n",
    "    data = vectorize_data(data, ngrams, binn, idf)\n",
    "    return partition_data(data, labels)\n",
    "\n",
    "def partition_data(data, labels, ratio = 0.7):\n",
    "    N = int(ratio * data.shape[0])\n",
    "    idx = np.random.permutation(data.shape[0])\n",
    "    train_data = data[idx[:N]]\n",
    "    train_labels = labels[idx[:N]]\n",
    "    test_data = data[idx[N:]]\n",
    "    test_labels = labels[idx[N:]]\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def get_data(files):\n",
    "    data = {}\n",
    "    for key in files:\n",
    "        file_data = parse_file(files[key])\n",
    "        tr_data, tr_labels, te_data, te_labels = parse_data(file_data)\n",
    "        partitioned_data = {\n",
    "            'train_data': tr_data,\n",
    "            'train_labels' : tr_labels,\n",
    "            'test_data': te_data,\n",
    "            'test_labels' : te_labels\n",
    "        }\n",
    "        data[key] = partitioned_data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(clfs, data):\n",
    "    for clf in clfs:\n",
    "        train_data = data['train_data']\n",
    "        train_labels = data['train_labels']\n",
    "        clf[0].fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "clfs = np.array([\n",
    "    [skl_svm.LinearSVC(random_state=0), 'SVM'],\n",
    "#    [skl_lm.LogisticRegression(), 'Max Entropy'],\n",
    "#    [skl_nb.MultinomialNB(alpha=.01), \"Multinomial NB\"],\n",
    "#    [skl_nb.BernoulliNB(alpha=.01), \"BernoulliNB\"],\n",
    "#    [skl_lm.RidgeClassifier(tol=1e-2, solver=\"sag\"), \"Ridge Classifier\"],\n",
    "#    [skl_lm.Perceptron(max_iter=50), \"Perceptron\"], \n",
    "#    [skl_lm.PassiveAggressiveClassifier(max_iter=50), \"Passive-Aggressive\"],\n",
    "#    [skl_nei.KNeighborsClassifier(n_neighbors=10), \"kNN\"],\n",
    "#    [skl_en.RandomForestClassifier(n_estimators=100), \"Random Forest\"]\n",
    "])\n",
    "\n",
    "all_data = get_data(data_files)\n",
    "\n",
    "for dataset_label in all_data:\n",
    "    train(clfs, all_data[dataset_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/objects/vectorizers/tfidf\", \"wb\") as file:\n",
    "    file.write(pickle.dumps(vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in clfs:\n",
    "    with open(\"../data/objects/trained_classifiers/\" + clf[1], \"wb\") as file:\n",
    "        file.write(pickle.dumps(clf[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in clfs:\n",
    "    with open(\"../data/objects/trained_classifiers/\" + clf[1], \"rb\") as file:\n",
    "        classifier = pickle.loads(b''.join(file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "     verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/objects/vectorizers/tfidf\", \"rb\") as file:\n",
    "    tmp = pickle.loads(b''.join(file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfe",
   "language": "python",
   "name": "pfe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
